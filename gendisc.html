<!doctype html>
<html>
    <head>
        <title>Generative VS Discriminative ML models</title>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <link href="./css/impress-common.css" rel="stylesheet" />
        <link href="./css/presentation.css" rel="stylesheet" />
    </head>
    <body>
        <div id="impress" data-transition-duration="500">
            <div id="Deep-field" class="step skip background" data-x="15000" data-y="3000" data-z="-500" data-scale="30">
                <img src="images/background.jpeg" width="900"/>
            </div>

            <div id="Overview" class="step" data-x="15000" data-y="2000" data-scale="20">
                <h1>
                    Generative Deep Neural Models
                </h1>
            </div>

            <div id="Agenda" class="step" data-x="5000" data-y="-4200">
                <h1>Agenda</h1>
                <ul>
                    <li>Discriminative VS Generative models</li>
                    <li>Embedding-space / manifold hypothesis</li>
                    <li>Generative approaches</li>
                    <ul>
                        <li>Variational Autoencoders</li>
                        <li>Normalizing Flows</li>
                        <li>Denoising Diffusion Probabilistic Models</li>
                        <li>Generative Adversarial Networks</li>
                    </ul>
                    <li>Performances</li>
                    <li>New Generative opportunities</li>
                    <li>Conclusions</li>
                </ul>
            </div>

            <!-- Definition -->

            <div id="Discriminative" class="step" data-x="4000" data-y="-2000">
                <h1>Discriminative models</h1>
                <center>
                    Given some pairs $\mathcal{XY}=\{(x_i,y_i)\}$... what about learning:
                    $$f:\mathcal{X}\to\mathcal{Y} = f(x) = \arg\max_yp(y|x)$$
                </center>
                <img src="images/discriminative.png" width="500"/>
                <did>Maximum posterior approach.</did>
            </div>

            <div id="Generative" class="step" data-rel-x="1000" data-rel-y="0">
                <h1>Generative models</h1>
                <center>
                    What about learning the per-class conditional distributions?
                    $$f:\mathcal{X}\to\mathcal{Y} = f(x) = \arg\max_yp(x|y)$$
                </center>
                <img src="images/generative.png" width="500"/>
                <did>Maximum likelihood approach.</did>
            </div>

            <div id="Disc-VS-Gen" class="step" data-rel-x="-500" data-rel-y="400" data-scale="0.5">
                <h1>Discriminative VS Generative</h1>
                <img src="images/discriminative_VS_generative.png" width="900"/>
                <h2>Partition VS fit<h2>
            </div>

            <!-- Embedding / manifold -->

            <div id="Manifold" class="step" data-rel-x="0" data-rel-y="600">
                <h1>Manifold hypothesis</h1>
                <img src="images/manifold.svg" width="900"/>
                <did>Samples lie on a lower dimensional manifold wrt the data space.</did>
            </div>

            <div id="Embedding" class="step" data-rel-x="1000" data-rel-y="0">
                <h1>Embedding space</h1>
                <img src="images/MNIST-tSNE.pbm" width="500"/>
                <did>
                    Samples can be mapped (from the manifold)<br/>
                    on a lower-dimensional (embedding) space<br/>
                    keeping (and disentangling) the latent characteristics.
                </did>
            </div>

            <!-- Approaches --> 

            <div id="Generative-approaches" class="step" data-rel-x="1000" data-rel-y="0">
                <h1>Generative approaches</h1>
                <img src="images/approaches.svg" width="900"/>
                <did>Complex distributions are mapped on simpler ones.</did>
            </div>

            <div id="VAE" class="step" data-rel-x="1000" data-rel-y="1000" data-rotate-z="45">
                <h1>Variational Auto-Encoders</h1>
                <img src="images/VAE.svg" width="800"/>
                <did>$$\log p(x) \ge \mathcal{L}(x) = \mathbb{E}_{q(x|z)}\log p(x|z) - KL(q(z|x)||p(z))$$</did>
            </div>

            <div id="NF" class="step" data-rel-x="0" data-rel-y="1500" data-rotate-z="135">
                <h1>Normalizing Flows</h1>
                <p>
                    Being $\{f_i\}$ diffeomorphisms:<tab/>
                    $p_{\mathcal{Z}_i}(z_i)=p_{\mathcal{Z}_{i+1}}(f_i(z_i))|\det Jf_i(z_i)|$
                </p>
                <a href="images/flow.gif" target="_blank">
                    <img src="images/nf_deform.png" width="350"/>
                </a>
                <p style="margin-top: 0;">
                    <img src="images/x.svg" width="100" style="display: inline-block; margin: 0;"/>
                    <span style="width: 70%; display: inline-block; margin: 0 10px;">
                        $$x = z_0 \rightarrow f_1(z_0) = z_1 \rightarrow \cdots \rightarrow f_n(z_{n-1}) = z_n$$
                        $$x = z_0 = f_1^{-1}(z_1) \leftarrow \cdots \leftarrow z_{n-1} = f_n^{-1}(z_n)$$
                    </span>
                    <img src="images/z.svg" width="100" style="display: inline-block; margin: 0;"/>
                </p>
            </div>

            <div id="DDPM" class="step" data-rel-x="-1500" data-rel-y="0" data-rotate-z="225">
                <h1>Denoising Diffusion Probabilistic Models</h1>
                <p>
                    <img src="images/x.svg" width="100" style="display: inline-block; margin: 0; vertical-align: middle;"/>
                    <img src="images/diffusion_model.png" width="666" style="display: inline-block; vertical-align: middle;"/>
                    <img src="images/z.svg" width="100" style="display: inline-block; margin: 0; vertical-align: middle;"/>
                </p>
                <did>Reaching the normal distribution via <a href="images/diffusion.gif" target="_blank">Brownian motion</a>.</did>
                <img src="images/DDPM.png" width="900"/>
            </div>

            <div id="GAN" class="step" data-rel-x="0" data-rel-y="-1500" data-rotate-z="315">
                <h1>Generative Adversarial Networks</h1>
                <p>
                    Train directly a generator network $G$...<br/>
                    <ptab/>...using a fancy loss function with a classifier $D$.
                </p>
                <p>
                    <img src="images/GAN.svg" width="400" style="display: inline-block; vertical-align: middle;"/>
                    <span style="display: inline-block;">
                    $$\small
                    \begin{eqnarray}
                        \mathcal{V}(G,D)&=&\min_G\max_D 
                        \mathbb{E}_x\left[\log D(x)\right] \\
                        &+& \mathbb[E]_z\left[\log(1-D(G(z)))\right]
                    \end{eqnarray}
                    $$
                    </span>
                </p>
            </div>

            <div id="Classificaiton-performances" class="step" data-rel-x="3000" data-rel-y="0" data-rotate-z="360">
                <h1>Classification performances</h1>
                <p>
                    Samples complexity to reach the asymptotic error...<br/>
                    <ttab/>...in case of Vapnik-Chervonenkis dimension $n$:
                </p>
                <table style="width: 100%; margin-bottom: 30px;"><tr>
                    <td>Logistic regression: $O(n)$<td/>
                    <td>Naive Bayes: $O(\log n)$</td>
                </tr></table>
                <img src="images/class-perf.png" width="900"/>
                <center>BUT</center>
                $$\epsilon(h_{Dis,\infty}) \lt \epsilon(h_{Gen,\infty})$$
                <cit>
                    <a href="https://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf">
                        "On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes", 
                        Andrew Y. Ng, Michael I. Jordan
                    </a>
                </cit>
            </div>

            <div id="Generation-quality" class="step" data-rel-x="1000" data-rel-y="-1000" data-rotate-z="270">
                <h1>Generation quality</h1>
                Hard task, currently a research topic.
                Some common metric issues:
                <dl>
                    <dt>Slow</dt>
                    <dd>Biased, comparing distributions requires many samples (~50k).</dd>
                    <dt>Perceptual quality</dt>
                    <dd>Difficulty assessing blurriness, noise, artefacts, ...</dd>
                    <dt>Different media</dt>
                    <dd>Do not work only with images: consider videos, text, audio, ...</dd>
                </dl>
                <img src="images/PPL.png" width="700"/>
                <cit>
                    <a href="https://arxiv.org/pdf/2103.09396.pdf">
                        "Pros and Cons of GAN Evaluation Measures: New Developments", Ali Borji
                    </a>
                </cit>
            </div>

            <div id="Distribution-adherence" class="step" data-rel-x="-1000" data-rel-y="-1000" data-rotate-z="180">
                <h1>Distribution adherence</h1>
                Some common generator issues:
                <dl>
                    <dt>Memorization</dt>
                    <dd>The model returns samples too similar to the training set.</dd>
                    <dt>Mode collapse</dt>
                    <dd>All generated samples are related to a restricted set of modes.</dd>
                    <dt>Off manifold</dt>
                    <dd>Generated samples lie outside the data manifold (eg. interpolation).</dd>
                </dl>
                <img src="images/prerec.png" width="900"/>
                <cit>
                    <a href="https://arxiv.org/pdf/2103.09396.pdf">
                        "Pros and Cons of GAN Evaluation Measures: New Developments", Ali Borji
                    </a>
                </cit>
            </div>

            <div id="Embedding-quality" class="step" data-rel-x="-1000" data-rel-y="1000" data-rotate-z="90">
                <h1>Embedding quality</h1>
                Some desirable aspects of the empedding space:
                <dl>
                    <dt>Clustering</dt>
                    <dd>The embedding clusters samples with similar characteristics.</dd>
                    <dt>Compression</dt>
                    <dd>Data compressed without significant information loss.</dd>
                    <dt>Disentanglement</dt>
                    <dd>Latent variables change different characteristics of data.</dd>
                </dl>
                <img src="images/emotions.png" width="800"/>
                <cit>
                    <a href="https://deepgenerativemodels.github.io/assets/slides/cs236_lecture15.pdf">
                        "Deep generative models", Standford, 2021, CS236 - Lecture 15, Stefano Ermon, Yang Song
                    </a>
                </cit>
                <cit>
                    <a href="https://arxiv.org/pdf/2205.06102v1.pdf">
                        "Tensor-based Emotion Editing in the StyleGAN Latent Space", Rene Haas, Stella Graßhof, and Sami S. Brandt
                    </a>
                </cit>
            </div>

            <div class="step" data-x="27000" data-y="8000">
                Last slide
            </div>

        </div>

        <div id="impress-toolbar"></div>
        <div class="impress-progressbar"><div></div></div>
        <div class="impress-progress"></div>

        <script type="text/x-mathjax-config">
          MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['$','$'], ['\\(','\\)'] ]
          },
          TeX: {
            Macros: {
              energy: "e",
              eqby: ["\\stackrel{#1}{=}",1],
            }
          }
          });
        </script>
        <script type="text/javascript" src="js/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/javascript" src="js/impress.js"></script>

        <script>impress().init();</script>
    </body>
</html>
