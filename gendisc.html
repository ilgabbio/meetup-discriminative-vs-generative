<!doctype html>
<html>
    <head>
        <title>Generative VS Discriminative ML models</title>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <link href="./css/impress-common.css" rel="stylesheet" />
        <link href="./css/presentation.css" rel="stylesheet" />
    </head>
    <body>
        <div id="impress" data-transition-duration="500" data-width="1024" data-height="768" data-max-scale="3">
            <div id="Deep-field" class="step skip background" data-x="15000" data-y="3000" data-z="-500" data-scale="30">
                <img src="images/background.jpeg" width="900"/>
            </div>

            <div id="Overview" class="step" data-x="15000" data-y="2000" data-scale="20">
                <h1>
                    Generative Deep Neural Models
                </h1>
            </div>

            <div id="Agenda" class="step agenda" data-x="5000" data-y="-4200">
                <h1>Agenda</h1>
                <ul>
                    <li>Discriminative VS Generative models</li>
                    <li>Generative approaches</li>
                    <li>Performances</li>
                    <li>Applications</li>
                    <li>Text-to-image generators</li>
                </ul>
            </div>

            <!-- Definition -->

            <div id="Discriminative" class="step" data-x="7000" data-y="0">
                <h1>Discriminative models</h1>
                <center>
                    Given $\{(x_i,z_i)\}$ such that $x_i\in\Re$, $z_i\in\{0,1\}$, learn:
                    $$f:\mathcal{X}\to\mathcal{Z} = f(x) = \arg\max_zp(z|x)$$
                </center>
                <img src="images/discriminative.png" width="500"/>
                <did>Maximum posterior approach.</did>
            </div>

            <div id="Generative" class="step" data-rel-x="0" data-rel-y="0">
                <h1>Generative models</h1>
                <center>
                    Learn the per-class conditional distributions $p(x|z)$:
                    $$f:\mathcal{X}\to\mathcal{Z} = f(x) = \arg\max_zp(x|z)$$
                </center>
                <img src="images/generative.png" width="500"/>
                <did>Maximum likelihood approach.</did>
            </div>

            <div id="Disc-VS-Gen" class="step" data-rel-x="0" data-rel-y="170" data-scale="0.1">
                <h1>Discriminative VS Generative</h1>
                <img src="images/discriminative_VS_generative.png" width="900"/>
                <h2 style="margin-top: 40px;">
                    Partition VS fit
                <h2>
            </div>

            <!-- Embedding / manifold -->

            <div id="Manifold" class="step" data-rel-x="0" data-rel-y="600">
                <h1>Manifold hypothesis</h1>
                <center>
                    <img src="images/manifold.svg" height="500" style="display: inline-block; vertical-align: middle;"/>
                    <img src="images/MNIST-tSNE.pbm" height="100" style="display: inline-block; vertical-align: middle;"/>
                </center>
                <did>Samples lie on a lower dimensional manifold wrt the data space.</did>
            </div>

            <div id="Embedding" class="step" data-rel-x="0" data-rel-y="0">
                <h1>Embedding space</h1>
                <center>
                    <img src="images/manifold.svg" height="100" style="display: inline-block; vertical-align: middle;"/>
                    <img src="images/MNIST-tSNE.pbm" height="500" style="display: inline-block; vertical-align: middle;"/>
                </center>
                <did>
                    Samples can be mapped (from the manifold)<br/>
                    on a lower-dimensional (embedding) space<br/>
                    keeping (and disentangling) the latent characteristics.
                </did>
            </div>

            <!-- Approaches --> 

            <div id="Generative-approaches" class="step" data-rel-x="1000" data-rel-y="0">
                <h1>Generative approaches</h1>
                <img src="images/approaches.svg" width="900"/>
                <did>Complex distributions are mapped on simpler ones.</did>
            </div>

            <div id="VAE" class="step" data-rel-x="0" data-rel-y="1000">
                <h1>Variational Auto-Encoders</h1>
                <img src="images/VAE.svg" width="800"/>
                $$
                    \log p(x) \ge ELBO(x) = \mathbb{E}_{q(x|z)}\log p(x|z) - KL(q(z|x)||p(z))
                $$
            </div>

            <div id="VAE-detail" class="step" data-rel-x="300" data-rel-y="170" data-scale="0.1">
                <h1>Normally distributed latents</h1>
                $$
                    \mathcal{L}(x,\hat{x}) =
                        \sum \|x - \hat{x}\|^2 - \frac{1}{2}
                        \sum \left[
                            1 + \log \sigma_i^2 - \sigma_i^2 - \mu_i^2
                        \right]
                $$
                <img src="images/vae_forces.png" width="445" style="background: white; display: inline-block; margin: 0 auto;"/>
                <img src="images/vae_space.webp" width="445" style="background: white; display: inline-block; margin: 0 auto;"/>
                <issue>Generated images are blurry</issue>
            </div>

            <div id="Autoregressive" class="step" data-rel-x="1200" data-rel-y="1330" data-rotate-z="45">
                <h1>Autoregressive models</h1>
                $$
                    p(x_0,\cdots,x_n) = p(x_0) \: p(x_1|x_0) \: p(x2|x_0,x_1) \cdots p(x_n|x_0,\cdots,x_{n-1})
                $$
                <center>
                    <img style="display: inline-block;" src="images/PixCNNprob.png" height="300"/>
                    <img style="display: inline-block;" src="images/PixCNN_VQ.png" height="300"/>
                </center>
                <div style="margin: 20px auto; width: 70%;">
                    Quantized:
                    <ul>
                        <li>a <b>vocabulary</b> of embedding-vectors is trained </li>
                        <li><b>keys</b> are generated instead of pixels</li>
                        <li>a <b>decoder</b> generates images from vectors</li>
                    </ul>
                </div>
            </div>

            <div id="VQ-VAE" class="step" data-rel-x="-220" data-rel-y="220" data-rotate-z="45" data-scale="0.1">
                <h1>Vector-Quantized VAE</h1>
                <img src="images/VQVAE.png" width="900" style="margin: 30px auto;"/>
                $$
                    \begin{eqnarray}
                        \mathcal{E}(x,e) &=& \|sg[e]-E[x]\|^2 & \text{Good encoder} \\
                        \mathcal{C}(x,e) &=& \|e-sg[E(x)]\|^2 & \text{Good codebook} \\
                        \mathcal{D}(x,e) &=& \|x-D(e)\|^2 & \text{Good decoder} \\
                    \end{eqnarray}
                $$
                $$\large
                    \mathcal{L}(x,e) = \mathcal{E}(x,e) + \mathcal{C}(x,e) + \beta\mathcal{D}(x,e)
                $$
                <issue>Slow, limited in token resolution</issue>
            </div>

            <div id="GAN" class="step" data-rel-x="220" data-rel-y="1280" data-rotate-z="135">
                <h2>Generative Adversarial Networks</h2>
                <p>
                    Train directly a generator network $G$...<br/>
                    <ptab/>...using a fancy loss function based on a classifier $D$.
                </p>
                <img src="images/GAN.svg" width="800"/>
                $$\small
                    \mathcal{V}(G,D)=\min_G\max_D 
                    \mathbb{E}_x\left[\log D(x)\right] + \mathbb{E}_z\left[\log(1-D(G(z)))\right]
                $$
            </div>

            <div id="GAN-VQ" class="step" data-rel-x="-220" data-rel-y="-220" data-rotate-z="135" data-scale="0.1">
                <h1>Vector-Quantized GAN</h1>
                <img src="images/VQGAN.jpg" width="900"/>
                <a href="https://arxiv.org/pdf/2012.09841.pdf">
                    <cit>"Taming Transformers for High-Resolution Image Synthesis",
                         Patrick Esser, Robin Rombach, Bjorn Ommer, Jun 2021</cit>
                <a/>
                $$
                    \mathcal{Q} = \arg\min_{E,G,\mathcal{Z}}\max_D
                    \mathbb{E}\left[
                        \mathcal{L}_{VQ}(E,G,\mathcal{Z}) + \lambda\mathcal{L}_{GAN}(\{E,G,\mathcal{Z}\},D)
                    \right]
                $$
                NOTE: Instead of MSE a <i>perceptual loss</i> is used.
            </div>

            <div id="GAN-detail" class="step skip" data-rel-x="-220" data-rel-y="-220" data-rotate-z="135" data-scale="0.1">
                <h1>Training issues</h1>
                <dl>
                    <dt>Vanishing gradients</dt>
                    <dd>
                        <img src="images/gan_vanishing_gradient.svg" width="700"/>
                        <did>What about a very bad generator and a very good discriminator?</did>
                    </dd>
                    
                    <dt>Sample diversity</dt>
                    <dd>
                        <img src="images/gan_mode_collapse.svg" width="700"/>
                        <did>How to enforce full distribution support coverage?</did>
                    </dd>
                    
                    <dt>Convergence failure</dt>
                    <dd>
                        Zero-sum game: Generator improves while Discriminator degrades.
                        <a href="https://arxiv.org/pdf/2002.09124.pdf" target="_blank">
                            <cit>"GANs May Have No Nash Equilibria", Farzan Farnia, Asuman Ozdaglar, MIT</cit>
                        </a>
                    </dd>
                </dl>
                <issue>Not easy to train and bring to convergence</issue>
            </div>

            <div id="NF" class="step" data-rel-x="-1060" data-rel-y="440" data-rotate-z="225">
                <h1 style="margin: 0;">Normalizing Flows</h1>
                <p style="margin: 0;">
                    <img src="images/x.svg" width="100" style="display: inline-block; margin: 0;"/>
                    <span style="width: 70%; display: inline-block; margin: 0 10px;">
                        $$x = z_0 \rightarrow f_1(z_0) = z_1 \rightarrow \cdots \rightarrow f_n(z_{n-1}) = z_n$$
                        $$x = z_0 = f_1^{-1}(z_1) \leftarrow \cdots \leftarrow z_{n-1} = f_n^{-1}(z_n)$$
                    </span>
                    <img src="images/z.svg" width="100" style="display: inline-block; margin: 0;"/>
                </p>
                <p style="margin-top: 0;">
                    being $\{f_i\}$ diffeomorphisms:<tab/>
                    $p_{\mathcal{Z}_i}(z_i)=p_{\mathcal{Z}_{i+1}}(f_i(z_i))|\det Jf_i(z_i)|$
                </p>
                <img src="images/nf_moons.png" width="460" style="display: inline-block; vertical-align: middle;"/>
                <a href="images/flow.gif" target="_blank">
                    <img src="images/nf_deform.png" width="400" style="display: inline-block; vertical-align: middle;"/>
                </a>
            </div>

            <div id="NF-detail" class="step skip" data-rel-x="-150" data-rel-y="-100" data-rotate-z="225" data-scale="0.1">
                <h1>The loss function</h1>
                $$
                    \begin{eqnarray}
                        \theta &=& \arg\max_\theta \log p_\mathcal{X}(\{x_i\}) \\
                               &=& \arg\max_\theta 
                                \sum_i\log p_\mathcal{Z}\left(f(x_i)\right)+\sum_k\log\left|\det\left[Jf_k(x_i)\right]\right|
                    \end{eqnarray}
                $$
                <p>
                    The challenge: flexibility + efficiency:
                </p>
                <img src="images/NF_coupling.svg" width="500"/>
                <did>Coupling flow: arbitrary complexity</did>
                <issue>Cannot change the number of variables</issue>
            </div>

            <div id="DDPM" class="step" data-rel-x="150" data-rel-y="-1400" data-rotate-z="315">
                <h1>Denoising Diffusion Probabilistic Models</h1>
                <p>
                    <img src="images/x.svg" width="100" style="display: inline-block; margin: 0; vertical-align: middle;"/>
                    <img src="images/diffusion_model.png" width="666" style="display: inline-block; vertical-align: middle;"/>
                    <img src="images/z.svg" width="100" style="display: inline-block; margin: 0; vertical-align: middle;"/>
                </p>
                <did>Reaching the normal distribution via <a href="images/diffusion.gif" target="_blank">Brownian motion</a>.</did>
                <img src="images/DDPM.png" width="900"/>
            </div>

            <div id="DDPM-detail" class="step" data-rel-x="200" data-rel-y="-500" data-rotate-z="315" data-scale="0.1">
                <h1>Score-based generative models</h1>
                $$\small
                    x_t \leftarrow x_{t-1} + \alpha\nabla_x\log p(x_{t-1}) + \sqrt{2\alpha}z_t \qquad z_t\text{white noise}
                $$
                <img src="images/hill_climbing.svg" width="700"/>
                <did>Annealed Langevin dynamics for a $\sigma_i$ schedule</did>
                $$\small
                    p_{\sigma_i}(\tilde{x}|x) = \mathcal{N}(\tilde{x}|x,\sigma_i I) \qquad
                    \nabla_x \log p_{\sigma_i}(\tilde{x}|x) \approx \text{NN} s_\theta(x,\sigma_i)
                $$
                <img src="images/diffusion_beat_GANs.png" width="900"/>
                <a href="https://arxiv.org/pdf/2105.05233.pdf" target="_blank">
                    <cit>"Diffusion Models Beat GANs on Image Synthesis", Prafulla Dhariwal, Alex Nichol</cit>
                </a>
                <issue>High evaluation time</issue>
            </div>

            <div id="DDPM-SDE" class="step" data-rel-x="170" data-rel-y="170" data-rotate-z="315" data-scale="0.1">
                <h1>Continuous DDPM using SDEs</h1>
                $$
                    \large
                    dx = f(x,t)dt + g(t)dw \\
                    \scriptsize
                    \begin{eqnarray}
                        f(\cdot):&& \text{Drift vector} \\
                        t:&& \text{Time} \\
                        g(\cdot):&& \text{Diffusion factor} \\
                        w:&& \text{Brownian motion} \\
                    \end{eqnarray}
                $$
                <img src="images/DDPM_SDE.png" width="900"/>
                <a href="https://arxiv.org/pdf/2011.13456.pdf" target="_blank">
                    <cit>"SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS",
                         Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole</cit>
                </a>
                <issue>High evaluation time</issue>
            </div>

            <!-- Evaluation -->

            <div id="Classificaiton-performances" class="step skip" data-rel-x="3370" data-rel-y="-2330" data-rotate-z="360">
                <h1>Classification performances</h1>
                <p>
                    Samples complexity to reach the asymptotic error...<br/>
                    <ttab/>...in case of Vapnik-Chervonenkis dimension $n$:
                </p>
                <table style="width: 100%; margin-bottom: 30px;"><tr>
                    <td>Logistic regression: $O(n)$<td/>
                    <td>Naive Bayes: $O(\log n)$</td>
                </tr></table>
                <img src="images/class-perf.png" width="900"/>
                <center>BUT</center>
                $$\epsilon(h_{Dis,\infty}) \lt \epsilon(h_{Gen,\infty})$$
                <a href="https://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf" target="_blank">
                    <cit>"On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes", Andrew Y. Ng, Michael I. Jordan</cit>
                </a>
            </div>

            <div id="Generation-quality" class="step skip" data-rel-x="1000" data-rel-y="-1000" data-rotate-z="270">
                <h1>Generation quality</h1>
                Hard task, currently a research topic.
                Some common metric issues:
                <dl>
                    <dt>Slow</dt>
                    <dd>Biased, comparing distributions requires many samples (~50k).</dd>
                    <dt>Perceptual quality</dt>
                    <dd>Difficulty assessing blurriness, noise, artefacts, ...</dd>
                    <dt>Different media</dt>
                    <dd>Do not work only with images: consider videos, text, audio, ...</dd>
                </dl>
                <img src="images/PPL.png" width="700"/>
                <a href="https://arxiv.org/pdf/2103.09396.pdf" target="_blank">
                    <cit>"Pros and Cons of GAN Evaluation Measures: New Developments", Ali Borji</cit>
                </a>
            </div>

            <div id="Distribution-adherence" class="step skip" data-rel-x="-1000" data-rel-y="-1000" data-rotate-z="180">
                <h1>Distribution adherence</h1>
                Some common generator issues:
                <dl>
                    <dt>Memorization</dt>
                    <dd>The model returns samples too similar to the training set.</dd>
                    <dt>Mode collapse</dt>
                    <dd>All generated samples are related to a restricted set of modes.</dd>
                    <dt>Off manifold</dt>
                    <dd>Generated samples lie outside the data manifold (eg. interpolation).</dd>
                </dl>
                <img src="images/prerec.png" width="900"/>
                <a href="https://arxiv.org/pdf/2103.09396.pdf" target="_blank">
                    <cit>"Pros and Cons of GAN Evaluation Measures: New Developments", Ali Borji</cit>
                </a>
            </div>

            <div id="Embedding-quality" class="step skip" data-rel-x="-1000" data-rel-y="1000" data-rotate-z="90">
                <h1>Embedding quality</h1>
                Some desirable aspects of the empedding space:
                <dl>
                    <dt>Clustering</dt>
                    <dd>The embedding clusters samples with similar characteristics.</dd>
                    <dt>Compression</dt>
                    <dd>Data compressed without significant information loss.</dd>
                    <dt>Disentanglement</dt>
                    <dd>Latent variables change different characteristics of data.</dd>
                </dl>
                <img src="images/emotions.png" width="800"/>
                <a href="https://deepgenerativemodels.github.io/assets/slides/cs236_lecture15.pdf" target="_blank">
                    <cit>"Deep generative models", Standford, 2021, CS236 - Lecture 15, Stefano Ermon, Yang Song</cit>
                </a>
                <a href="https://arxiv.org/pdf/2205.06102v1.pdf" target="_blank">
                    <cit>"Tensor-based Emotion Editing in the StyleGAN Latent Space", Rene Haas, Stella Graßhof, and Sami S. Brandt</cit>
                </a>
            </div>

            <div id="Applications01" class="step" data-rel-x="500" data-rel-y="2500" data-rotate-z="45">
                <h1>A new world of applications</h1>
                <dl>
                    <dt>Capturing the distribution of data:</dt>
                    <dd>
                        sample
                        <span style="display: inline-block; vertical-align: middle;">
                            <img src="images/manga.png" width="200"/>
                        </span>
                        <tab/>
                        project
                        <span style="display: inline-block; vertical-align: middle;">
                            <img src="images/gab.png" width="200"/>
                        </span>
                        <span style="display: inline-block; vertical-align: middle;">
                            <img src="images/gab_proj.png" width="200"/>
                        </span>
                    </dd>
                    
                    <dt>Detecting outliers/anomalies:</dt>
                    <dd>
                        <img src="images/AnoVAE.png" width="900"/>
                        <a href="https://arxiv.org/pdf/2004.03271.pdf">
                            <cit>"Autoencoders for Unsupervised Anomaly Segmentation in Brain MR Images: A Comparative Study",
                                 Christoph Baur, Stefan Denner, Benedikt Wiestler, Shadi Albarqouni and Nassir Navab</cit>
                        </a>
                    </dd>
                </dl>
            </div>

            <div id="Applications02" class="step skip" data-rel-x="1500" data-rel-y="0" data-rotate-z="45">
                <h1>A new world of applications</h1>
                <dl>
                    <dt>Latent-space feature editing:</dt>
                    <dd>
                        <img src="images/InterFaceGAN.png" width="900"/>
                        <a href="https://arxiv.org/pdf/1907.10786.pdf">
                            <cit>"Interpreting the Latent Space of GANs for Semantic Face Editing",
                                 Yujun Shen, Jinjin Gu, Xiaoou Tang, Bolei Zhou</cit>
                        </a>
                    </dd>
                </dl>
            </div>

            <div id="Applications03" class="step skip" data-rel-x="1500" data-rel-y="0" data-rotate-z="45">
                <h1>A new world of applications</h1>
                <dl>
                    <dt>Image transformation via conditional generation:</dt>
                    <dd>
                        <img src="images/pix2pix.jpeg" width="900"/>
                        <a href="https://arxiv.org/pdf/1611.07004.pdf">
                            <cit>"Image-to-Image Translation with Conditional Adversarial Nets",
                                 Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros</cit>
                        </a>
                    </dd>
                </dl>
            </div>

            <div id="Applications04" class="step skip" data-rel-x="1500" data-rel-y="0" data-rotate-z="45">
                <h1>A new world of applications</h1>
                <dl>
                    <dt>Aerial images semantic segmentation:</dt>
                    <dd>
                        <img src="images/pix2pix_semseg_aerial.png" height="200"/>
                        <a href="https://github.com/A2Amir/Pix2Pix-for-Semantic-Segmentation-of-Satellite-Images" target="_blank">
                            <cit>"What Is Pix2Pix and How To Use It for Semantic Segmentation of Satellite Images?"</cit>
                        </a>
                    </dd>

                    <dt>Cell membrane and nucleus semantic segmentation:</dt>
                    <dd>
                        <img src="images/pix2pix_semseg_cell.png" width="900"/>
                        <a href="https://www.researchgate.net/profile/Hiroki_Tsuda4/publication/340555405_Cell_Image_Segmentation_by_Integrating_Pix2pixs_for_Each_Class/links/5f45ba7ba6fdcccc43057ea6/Cell-Image-Segmentation-by-Integrating-Pix2pixs-for-Each-Class.pdf?origin=publication_detail">
                            <cit>"Cell Image Segmentation by Integrating Pix2pixs for Each Class",
                                 Hiroki Tsuda and Kazuhiro Hotta</cit>
                        </a>
                    </dd>
                </dl>
            </div>

            <div id="Applications05" class="step skip" data-rel-x="-1500" data-rel-y="1500" data-rotate-z="45">
                <h1>A new world of applications</h1>
                <dl>
                    <dt>Domain adaptation</dt>
                    <dd>
                        <img src="images/AttentionGAN.png" width="800"/>
                        <a href="https://arxiv.org/pdf/1911.11897.pdf" target="_blank">
                            <cit>"AttentionGAN: Unpaired Image-to-Image Translation using Attention-Guided Generative Adversarial Networks", Hao Tang, Hong Liu, Dan Xu, Philip H.S. Torr, and Nicu Sebe</cit>
                        </a>
                    </dd>
                </dl>
            </div>

            <div id="Applications06" class="step" data-rel-x="-1500" data-rel-y="1500" data-rotate-z="45">
                <h1>Super-resolution</h1>
                <img src="images/SRes.png" width="500"/>
                <img src="images/SResCascade.png" width="500"/>
                <center>
                    <a href="https://arxiv.org/pdf/2104.07636.pdf">
                        <cit>"Image Super-Resolution via Iterative Refinement", GoogleAI, 2021</cit>
                    </a>
                </center>
            </div>

            <div id="Text-to-images" class="step" data-rel-x="-1500" data-rel-y="1500" data-rotate-z="45">
                <h2 style="margin: 0 0;">Text-to-images, short story</h2>
                <div id="LastSupper" style="width: 70%; margin: 0 auto; overflow-x: scroll;">
                    <a href="images/LastSupper.jpeg" target="_blank">
                        <img src="images/LastSupper.jpeg" height="320"/>
                    </a>
                </div>
                <script>
                    function adjustLastSupper() { 
                        var elem = document.getElementById("LastSupper");
                        var elemWidth = elem.scrollWidth;
                        var elemVisibleWidth = elem.offsetWidth;
                        elem.scrollLeft = (elemWidth - elemVisibleWidth) * 28 / 50;
                    }
                </script>
                <a href="https://mobile.twitter.com/gabe_ragland/status/1539005324983578624/photo/1" target="_blank">
                    <did>@DALL-E 2 Pics, Twitter</did>
                </a>
                <dl>
                    <dt>DALL-E, OpenAI, 2021-Feb-24, Autoregressive Model</dt>
                    <dd>
                        <a href="https://arxiv.org/pdf/2102.12092.pdf" target="_blank">
                            <cit>"Zero-Shot Text-to-Image Generation"</cit>
                        </a>
                    </dd>

                    <dt>GLIDE, OpenAI, 2021-Dec-20, Diffusion Model</dt>
                    <dd>
                        <a href="https://arxiv.org/pdf/2112.10741.pdf" target="_blank">
                            <cit>"GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models"</cit>
                        </a>
                    </dd>

                    <dt>DALL-E 2, OpenAI, 2022-Apr-13, Diffusion Model</dt>
                    <dd>
                        <a href="https://cdn.openai.com/papers/dall-e-2.pdf" target="_blank">
                            <cit>"Hierarchical Text-Conditional Image Generation with CLIP Latents"</cit>
                        </a>
                    </dd>
                </dl>
            </div>

            <div id="CLIP" class="step" data-rel-x="200" data-rel-y="300" data-rotate-z="45" data-scale="0.1">
                <h1>CLIP</h1>
                CLIP, OpenAI, 2021-Feb-26
                <a href="https://arxiv.org/pdf/2103.00020.pdf">
                    <did>Learning Transferable Visual Models From Natural Language Supervision</did>
                </a>
                <img src="images/CLIP.png" width="900"/>
            </div>

            <div id="Transformers" class="step" data-rel-x="-50" data-rel-y="50" data-rotate-z="45" data-scale="0.1">
                <h1>Encoders with Transformers</h1>
                <img src="images/transformer.svg" width="900"/>
                <center>
                    <a href="https://arxiv.org/pdf/1706.03762.pdf">
                        <cit>"Attention Is All You Need", GoogleAI, 2017</cit>
                    </a>
                </center>
            </div>

            <div id="DALL-E2" class="step" data-rel-x="-50" data-rel-y="50" data-rotate-z="45" data-scale="0.1">
                <h1>unCLIP (DALL-E 2)</h1>
                <img src="images/DALLE2.png" width="900"/>
                $$
                    p(x|y) = p(x,z_i|y) = p(x|z_i,y) \; p(z_i|y) \\
                    x\equiv\text{image} \quad y\equiv\text{caption} \quad z_i=f(x)\equiv\text{image embedding}
                $$
            </div>

            <div id="Imagen" class="step" data-rel-x="-1600" data-rel-y="1100" data-rotate-z="45">
                <h1>Imagen</h1>
                Google AI, 2022-May-23, Diffusion Model
                <a href="https://imagen.research.google/paper.pdf" target="_blank">
                    <cit>"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"</cit>
                </a>
            </div>

            <div id="Parti-overview" class="step" data-rel-x="1500" data-rel-y="0" data-rotate-z="45">
                <h1>Parti</h1>
                GoogleAI, 22-Jun-2022, Autoregressive Model
                <a href="https://arxiv.org/abs/2206.10789" target="_blank">
                    <cit>"Scaling Autoregressive Models for Content-Rich Text-to-Image Generation"</cit>
                </a>
                <img src="images/Parti_overview.jpeg" width="900"/>
            </div>

            <div id="Parti-scaling" class="step" data-rel-x="1500" data-rel-y="0" data-rotate-z="45">
                <h1>Parti</h1>
                <h2>Scaling up the model.. with >1B samples</h2>
                <img src="images/Parti_outcome.png" width="900"/>
                <did style="font-size: 100%">
                    A portrait photo of a kangaroo <br/>
                    wearing an orange hoodie and blue sunglasses <br/>
                    standing on the grass in front of the Sydney Opera House <br/>
                    holding a sign on the chest that says Welcome Friends!
                </did>
            </div>

            <div id="Conclusions" class="step" data-rel-x="1500" data-rel-y="0" data-rotate-z="45">
                <h1>Conclusions</h1>
                <h2>Generative models</h2>
                <center style="font-size: 150%;">
                    Does the effectiveness worth their complexity?<br/><br/>
                    They are rapidly becoming "human-like"...<br/>
                    ...is this the start of a new era?<br/><br/>
                    What about creative jobs like<br/> illustration, narrative.. development?
                </center>
                <center style="font-size: 500%; color: white;">
                    Qs?
                </center>
            </div>

            <div id="The-end" class="step end" data-x="15000" data-y="2000" data-scale="20">
                <h1>Generative Deep Neural Models</h1>
                <p><tab/></p>
                <p><tab/></p>
                <center style="font-size: 500%; color: white;">
                    Let's have a "Parti"!
                </center>
            </div>

            <div id="Empty" class="step end" data-x="15000" data-y="2000" data-scale="20">
            </div>

        </div>

        <div id="impress-toolbar"></div>
        <div class="impress-progressbar"><div></div></div>
        <div class="impress-progress"></div>

        <script type="text/x-mathjax-config">
          MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['$','$'], ['\\(','\\)'] ]
          },
          TeX: {
            Macros: {
              energy: "e",
              eqby: ["\\stackrel{#1}{=}",1],
            }
          }
          });
        </script>
        <script type="text/javascript" src="js/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/javascript" src="js/impress.js"></script>

        <script>
            impress().init();
            adjustLastSupper();
        </script>
    </body>
</html>
